\section{Class List}
Here are the classes, structs, unions and interfaces with brief descriptions\-:\begin{DoxyCompactList}
\item\contentsline{section}{{\bf Is\-Vector$<$ Vec\-Type $>$} \\*If value == true, then Vec\-Type is some sort of Armadillo vector or subview }{\pageref{structIsVector}}{}
\item\contentsline{section}{{\bf Is\-Vector$<$ arma\-::\-Col$<$ e\-T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1Col_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{{\bf Is\-Vector$<$ arma\-::\-Row$<$ e\-T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1Row_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{{\bf Is\-Vector$<$ arma\-::\-Sp\-Col$<$ e\-T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1SpCol_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{{\bf Is\-Vector$<$ arma\-::\-Sp\-Row$<$ e\-T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1SpRow_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{{\bf Is\-Vector$<$ arma\-::\-Sp\-Subview$<$ e\-T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1SpSubview_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{{\bf Is\-Vector$<$ arma\-::subview\-\_\-col$<$ e\-T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1subview__col_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{{\bf Is\-Vector$<$ arma\-::subview\-\_\-row$<$ e\-T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1subview__row_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-A\-M\-F$<$ Termination\-Policy\-Type, Initialization\-Rule\-Type, Update\-Rule\-Type $>$} \\*This class implements \doxyref{A\-M\-F}{p.}{classmlpack_1_1amf_1_1AMF} (alternating matrix factorization) on the given matrix V }{\pageref{classmlpack_1_1amf_1_1AMF}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-Average\-Initialization} \\*This initialization rule initializes matrix W and H to root of average of V with uniform noise }{\pageref{classmlpack_1_1amf_1_1AverageInitialization}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-Complete\-Incremental\-Termination$<$ Termination\-Policy $>$} }{\pageref{classmlpack_1_1amf_1_1CompleteIncrementalTermination}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-Incomplete\-Incremental\-Termination$<$ Termination\-Policy $>$} }{\pageref{classmlpack_1_1amf_1_1IncompleteIncrementalTermination}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-N\-M\-F\-A\-L\-S\-Update} \\*This class implements a method titled 'Alternating Least Squares' described in the paper 'Positive Matrix Factorization\-: A Non-\/negative Factor Model with Optimal Utilization of Error Estimates of Data Values' by P Paatero and U Tapper }{\pageref{classmlpack_1_1amf_1_1NMFALSUpdate}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-N\-M\-F\-Multiplicative\-Distance\-Update} \\*The multiplicative distance update rules for matrices W and H }{\pageref{classmlpack_1_1amf_1_1NMFMultiplicativeDistanceUpdate}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-N\-M\-F\-Multiplicative\-Divergence\-Update} \\*This follows a method described in the paper 'Algorithms for Non-\/negative Matrix Factorization' by D }{\pageref{classmlpack_1_1amf_1_1NMFMultiplicativeDivergenceUpdate}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-Random\-Acol\-Initialization$<$ p $>$} \\*This class initializes the W matrix of the \doxyref{A\-M\-F}{p.}{classmlpack_1_1amf_1_1AMF} algorithm by averaging p randomly chosen columns of V }{\pageref{classmlpack_1_1amf_1_1RandomAcolInitialization}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-Random\-Initialization} }{\pageref{classmlpack_1_1amf_1_1RandomInitialization}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-Simple\-Residue\-Termination} \\*This class implements a simple residue-\/based termination policy }{\pageref{classmlpack_1_1amf_1_1SimpleResidueTermination}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-Simple\-Tolerance\-Termination$<$ Mat\-Type $>$} \\*This class implements residue tolerance termination policy }{\pageref{classmlpack_1_1amf_1_1SimpleToleranceTermination}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-S\-V\-D\-Batch\-Learning} \\*This class implements S\-V\-D batch learning with momentum }{\pageref{classmlpack_1_1amf_1_1SVDBatchLearning}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-S\-V\-D\-Complete\-Incremental\-Learning$<$ Mat\-Type $>$} }{\pageref{classmlpack_1_1amf_1_1SVDCompleteIncrementalLearning}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-S\-V\-D\-Complete\-Incremental\-Learning$<$ arma\-::sp\-\_\-mat $>$} }{\pageref{classmlpack_1_1amf_1_1SVDCompleteIncrementalLearning_3_01arma_1_1sp__mat_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-S\-V\-D\-Incomplete\-Incremental\-Learning} }{\pageref{classmlpack_1_1amf_1_1SVDIncompleteIncrementalLearning}}{}
\item\contentsline{section}{{\bf mlpack\-::amf\-::\-Validation\-R\-M\-S\-E\-Termination$<$ Mat\-Type $>$} }{\pageref{classmlpack_1_1amf_1_1ValidationRMSETermination}}{}
\item\contentsline{section}{{\bf mlpack\-::bound\-::\-Ball\-Bound$<$ Vec\-Type, T\-Metric\-Type $>$} \\*Ball bound encloses a set of points at a specific distance (radius) from a specific point (center) }{\pageref{classmlpack_1_1bound_1_1BallBound}}{}
\item\contentsline{section}{{\bf mlpack\-::bound\-::\-H\-Rect\-Bound$<$ Power, Take\-Root $>$} \\*Hyper-\/rectangle bound for an L-\/metric }{\pageref{classmlpack_1_1bound_1_1HRectBound}}{}
\item\contentsline{section}{{\bf mlpack\-::cf\-::\-C\-F$<$ Factorizer\-Type $>$} \\*This class implements Collaborative Filtering (\doxyref{C\-F}{p.}{classmlpack_1_1cf_1_1CF}) }{\pageref{classmlpack_1_1cf_1_1CF}}{}
\item\contentsline{section}{{\bf mlpack\-::\-C\-L\-I} \\*Parses the command line for parameters and holds user-\/specified parameters }{\pageref{classmlpack_1_1CLI}}{}
\item\contentsline{section}{{\bf mlpack\-::decision\-\_\-stump\-::\-Decision\-Stump$<$ Mat\-Type $>$} \\*This class implements a decision stump }{\pageref{classmlpack_1_1decision__stump_1_1DecisionStump}}{}
\item\contentsline{section}{{\bf mlpack\-::det\-::\-D\-Tree} \\*A density estimation tree is similar to both a decision tree and a space partitioning tree (like a kd-\/tree) }{\pageref{classmlpack_1_1det_1_1DTree}}{}
\item\contentsline{section}{{\bf mlpack\-::distribution\-::\-Discrete\-Distribution} \\*A discrete distribution where the only observations are discrete observations }{\pageref{classmlpack_1_1distribution_1_1DiscreteDistribution}}{}
\item\contentsline{section}{{\bf mlpack\-::distribution\-::\-Gaussian\-Distribution} \\*A single multivariate Gaussian distribution }{\pageref{classmlpack_1_1distribution_1_1GaussianDistribution}}{}
\item\contentsline{section}{{\bf mlpack\-::distribution\-::\-Laplace\-Distribution} \\*The multivariate Laplace distribution centered at 0 has pdf }{\pageref{classmlpack_1_1distribution_1_1LaplaceDistribution}}{}
\item\contentsline{section}{{\bf mlpack\-::emst\-::\-D\-T\-B\-Rules$<$ Metric\-Type, Tree\-Type $>$} }{\pageref{classmlpack_1_1emst_1_1DTBRules}}{}
\item\contentsline{section}{{\bf mlpack\-::emst\-::\-D\-T\-B\-Stat} \\*A statistic for use with M\-L\-P\-A\-C\-K trees, which stores the upper bound on distance to nearest neighbors and the component which this node belongs to }{\pageref{classmlpack_1_1emst_1_1DTBStat}}{}
\item\contentsline{section}{{\bf mlpack\-::emst\-::\-Dual\-Tree\-Boruvka$<$ Metric\-Type, Tree\-Type $>$} \\*Performs the M\-S\-T calculation using the Dual-\/\-Tree Boruvka algorithm, using any type of tree }{\pageref{classmlpack_1_1emst_1_1DualTreeBoruvka}}{}
\item\contentsline{section}{{\bf mlpack\-::emst\-::\-Dual\-Tree\-Boruvka$<$ Metric\-Type, Tree\-Type $>$\-::\-Sort\-Edges\-Helper} \\*For sorting the edge list after the computation }{\pageref{structmlpack_1_1emst_1_1DualTreeBoruvka_1_1SortEdgesHelper}}{}
\item\contentsline{section}{{\bf mlpack\-::emst\-::\-Edge\-Pair} \\*An edge pair is simply two indices and a distance }{\pageref{classmlpack_1_1emst_1_1EdgePair}}{}
\item\contentsline{section}{{\bf mlpack\-::emst\-::\-Union\-Find} \\*A Union-\/\-Find data structure }{\pageref{classmlpack_1_1emst_1_1UnionFind}}{}
\item\contentsline{section}{{\bf mlpack\-::fastmks\-::\-Fast\-M\-K\-S$<$ Kernel\-Type, Tree\-Type $>$} \\*An implementation of fast exact max-\/kernel search }{\pageref{classmlpack_1_1fastmks_1_1FastMKS}}{}
\item\contentsline{section}{{\bf mlpack\-::fastmks\-::\-Fast\-M\-K\-S\-Rules$<$ Kernel\-Type, Tree\-Type $>$} \\*The base case and pruning rules for \doxyref{Fast\-M\-K\-S}{p.}{classmlpack_1_1fastmks_1_1FastMKS} (fast max-\/kernel search) }{\pageref{classmlpack_1_1fastmks_1_1FastMKSRules}}{}
\item\contentsline{section}{{\bf mlpack\-::fastmks\-::\-Fast\-M\-K\-S\-Stat} \\*The statistic used in trees with \doxyref{Fast\-M\-K\-S}{p.}{classmlpack_1_1fastmks_1_1FastMKS} }{\pageref{classmlpack_1_1fastmks_1_1FastMKSStat}}{}
\item\contentsline{section}{{\bf mlpack\-::gmm\-::\-Diagonal\-Constraint} \\*Force a covariance matrix to be diagonal }{\pageref{classmlpack_1_1gmm_1_1DiagonalConstraint}}{}
\item\contentsline{section}{{\bf mlpack\-::gmm\-::\-Eigenvalue\-Ratio\-Constraint} \\*Given a vector of eigenvalue ratios, ensure that the covariance matrix always has those eigenvalue ratios }{\pageref{classmlpack_1_1gmm_1_1EigenvalueRatioConstraint}}{}
\item\contentsline{section}{{\bf mlpack\-::gmm\-::\-E\-M\-Fit$<$ Initial\-Clustering\-Type, Covariance\-Constraint\-Policy $>$} \\*This class contains methods which can fit a \doxyref{G\-M\-M}{p.}{classmlpack_1_1gmm_1_1GMM} to observations using the E\-M algorithm }{\pageref{classmlpack_1_1gmm_1_1EMFit}}{}
\item\contentsline{section}{{\bf mlpack\-::gmm\-::\-G\-M\-M$<$ Fitting\-Type $>$} \\*A Gaussian Mixture Model (\doxyref{G\-M\-M}{p.}{classmlpack_1_1gmm_1_1GMM}) }{\pageref{classmlpack_1_1gmm_1_1GMM}}{}
\item\contentsline{section}{{\bf mlpack\-::gmm\-::\-No\-Constraint} \\*This class enforces no constraint on the covariance matrix }{\pageref{classmlpack_1_1gmm_1_1NoConstraint}}{}
\item\contentsline{section}{{\bf mlpack\-::gmm\-::\-Positive\-Definite\-Constraint} \\*Given a covariance matrix, force the matrix to be positive definite }{\pageref{classmlpack_1_1gmm_1_1PositiveDefiniteConstraint}}{}
\item\contentsline{section}{{\bf mlpack\-::hmm\-::\-H\-M\-M$<$ Distribution $>$} \\*A class that represents a Hidden Markov Model with an arbitrary type of emission distribution }{\pageref{classmlpack_1_1hmm_1_1HMM}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Cosine\-Distance} \\*The cosine distance (or cosine similarity) }{\pageref{classmlpack_1_1kernel_1_1CosineDistance}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Epanechnikov\-Kernel} \\*The Epanechnikov kernel, defined as }{\pageref{classmlpack_1_1kernel_1_1EpanechnikovKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Example\-Kernel} \\*An example kernel function }{\pageref{classmlpack_1_1kernel_1_1ExampleKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Gaussian\-Kernel} \\*The standard Gaussian kernel }{\pageref{classmlpack_1_1kernel_1_1GaussianKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Hyperbolic\-Tangent\-Kernel} \\*Hyperbolic tangent kernel }{\pageref{classmlpack_1_1kernel_1_1HyperbolicTangentKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Kernel\-Traits$<$ Kernel\-Type $>$} \\*This is a template class that can provide information about various kernels }{\pageref{classmlpack_1_1kernel_1_1KernelTraits}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Kernel\-Traits$<$ Cosine\-Distance $>$} \\*Kernel traits for the cosine distance }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01CosineDistance_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Kernel\-Traits$<$ Epanechnikov\-Kernel $>$} \\*Kernel traits for the Epanechnikov kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01EpanechnikovKernel_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Kernel\-Traits$<$ Gaussian\-Kernel $>$} \\*Kernel traits for the Gaussian kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01GaussianKernel_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Kernel\-Traits$<$ Laplacian\-Kernel $>$} \\*Kernel traits of the Laplacian kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01LaplacianKernel_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Kernel\-Traits$<$ Spherical\-Kernel $>$} \\*Kernel traits for the spherical kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01SphericalKernel_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Kernel\-Traits$<$ Triangular\-Kernel $>$} \\*Kernel traits for the triangular kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01TriangularKernel_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-K\-Means\-Selection$<$ Clustering\-Type $>$} }{\pageref{classmlpack_1_1kernel_1_1KMeansSelection}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Laplacian\-Kernel} \\*The standard Laplacian kernel }{\pageref{classmlpack_1_1kernel_1_1LaplacianKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Linear\-Kernel} \\*The simple linear kernel (dot product) }{\pageref{classmlpack_1_1kernel_1_1LinearKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Nystroem\-Method$<$ Kernel\-Type, Point\-Selection\-Policy $>$} }{\pageref{classmlpack_1_1kernel_1_1NystroemMethod}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Ordered\-Selection} }{\pageref{classmlpack_1_1kernel_1_1OrderedSelection}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Polynomial\-Kernel} \\*The simple polynomial kernel }{\pageref{classmlpack_1_1kernel_1_1PolynomialKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-P\-Spectrum\-String\-Kernel} \\*The p-\/spectrum string kernel }{\pageref{classmlpack_1_1kernel_1_1PSpectrumStringKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Random\-Selection} }{\pageref{classmlpack_1_1kernel_1_1RandomSelection}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Spherical\-Kernel} }{\pageref{classmlpack_1_1kernel_1_1SphericalKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kernel\-::\-Triangular\-Kernel} \\*The trivially simple triangular kernel, defined by }{\pageref{classmlpack_1_1kernel_1_1TriangularKernel}}{}
\item\contentsline{section}{{\bf mlpack\-::kmeans\-::\-Allow\-Empty\-Clusters} \\*Policy which allows K-\/\-Means to create empty clusters without any error being reported }{\pageref{classmlpack_1_1kmeans_1_1AllowEmptyClusters}}{}
\item\contentsline{section}{{\bf mlpack\-::kmeans\-::\-K\-Means$<$ Metric\-Type, Initial\-Partition\-Policy, Empty\-Cluster\-Policy $>$} \\*This class implements K-\/\-Means clustering }{\pageref{classmlpack_1_1kmeans_1_1KMeans}}{}
\item\contentsline{section}{{\bf mlpack\-::kmeans\-::\-Max\-Variance\-New\-Cluster} \\*When an empty cluster is detected, this class takes the point furthest from the centroid of the cluster with maximum variance as a new cluster }{\pageref{classmlpack_1_1kmeans_1_1MaxVarianceNewCluster}}{}
\item\contentsline{section}{{\bf mlpack\-::kmeans\-::\-Random\-Partition} \\*A very simple partitioner which partitions the data randomly into the number of desired clusters }{\pageref{classmlpack_1_1kmeans_1_1RandomPartition}}{}
\item\contentsline{section}{{\bf mlpack\-::kmeans\-::\-Refined\-Start} \\*A refined approach for choosing initial points for k-\/means clustering }{\pageref{classmlpack_1_1kmeans_1_1RefinedStart}}{}
\item\contentsline{section}{{\bf mlpack\-::kpca\-::\-Kernel\-P\-C\-A$<$ Kernel\-Type, Kernel\-Rule $>$} \\*This class performs kernel principal components analysis (Kernel P\-C\-A), for a given kernel }{\pageref{classmlpack_1_1kpca_1_1KernelPCA}}{}
\item\contentsline{section}{{\bf mlpack\-::kpca\-::\-Naive\-Kernel\-Rule$<$ Kernel\-Type $>$} }{\pageref{classmlpack_1_1kpca_1_1NaiveKernelRule}}{}
\item\contentsline{section}{{\bf mlpack\-::kpca\-::\-Nystroem\-Kernel\-Rule$<$ Kernel\-Type, Point\-Selection\-Policy $>$} }{\pageref{classmlpack_1_1kpca_1_1NystroemKernelRule}}{}
\item\contentsline{section}{{\bf mlpack\-::lcc\-::\-Local\-Coordinate\-Coding$<$ Dictionary\-Initializer $>$} \\*An implementation of Local Coordinate Coding (L\-C\-C) that codes data which approximately lives on a manifold using a variation of l1-\/norm regularized sparse coding; in L\-C\-C, the penalty on the absolute value of each point's coefficient for each atom is weighted by the squared distance of that point to that atom }{\pageref{classmlpack_1_1lcc_1_1LocalCoordinateCoding}}{}
\item\contentsline{section}{{\bf mlpack\-::\-Log} \\*Provides a convenient way to give formatted output }{\pageref{classmlpack_1_1Log}}{}
\item\contentsline{section}{{\bf mlpack\-::math\-::\-Range} \\*Simple real-\/valued range }{\pageref{classmlpack_1_1math_1_1Range}}{}
\item\contentsline{section}{{\bf mlpack\-::metric\-::\-I\-P\-Metric$<$ Kernel\-Type $>$} }{\pageref{classmlpack_1_1metric_1_1IPMetric}}{}
\item\contentsline{section}{{\bf mlpack\-::metric\-::\-L\-Metric$<$ Power, Take\-Root $>$} \\*The L\-\_\-p metric for arbitrary integer p, with an option to take the root }{\pageref{classmlpack_1_1metric_1_1LMetric}}{}
\item\contentsline{section}{{\bf mlpack\-::metric\-::\-Mahalanobis\-Distance$<$ Take\-Root $>$} \\*The Mahalanobis distance, which is essentially a stretched Euclidean distance }{\pageref{classmlpack_1_1metric_1_1MahalanobisDistance}}{}
\item\contentsline{section}{{\bf mlpack\-::mvu\-::\-M\-V\-U} \\*Meant to provide a good abstraction for users }{\pageref{classmlpack_1_1mvu_1_1MVU}}{}
\item\contentsline{section}{{\bf mlpack\-::naive\-\_\-bayes\-::\-Naive\-Bayes\-Classifier$<$ Mat\-Type $>$} \\*The simple Naive Bayes classifier }{\pageref{classmlpack_1_1naive__bayes_1_1NaiveBayesClassifier}}{}
\item\contentsline{section}{{\bf mlpack\-::nca\-::\-N\-C\-A$<$ Metric\-Type, Optimizer\-Type $>$} \\*An implementation of Neighborhood Components Analysis, both a linear dimensionality reduction technique and a distance learning technique }{\pageref{classmlpack_1_1nca_1_1NCA}}{}
\item\contentsline{section}{{\bf mlpack\-::nca\-::\-Softmax\-Error\-Function$<$ Metric\-Type $>$} \\*The \char`\"{}softmax\char`\"{} stochastic neighbor assignment probability function }{\pageref{classmlpack_1_1nca_1_1SoftmaxErrorFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::neighbor\-::\-Furthest\-Neighbor\-Sort} \\*This class implements the necessary methods for the Sort\-Policy template parameter of the \doxyref{Neighbor\-Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} class }{\pageref{classmlpack_1_1neighbor_1_1FurthestNeighborSort}}{}
\item\contentsline{section}{{\bf mlpack\-::neighbor\-::\-L\-S\-H\-Search$<$ Sort\-Policy $>$} \\*The \doxyref{L\-S\-H\-Search}{p.}{classmlpack_1_1neighbor_1_1LSHSearch} class -- This class builds a hash on the reference set and uses this hash to compute the distance-\/approximate nearest-\/neighbors of the given queries }{\pageref{classmlpack_1_1neighbor_1_1LSHSearch}}{}
\item\contentsline{section}{{\bf mlpack\-::neighbor\-::\-Nearest\-Neighbor\-Sort} \\*This class implements the necessary methods for the Sort\-Policy template parameter of the \doxyref{Neighbor\-Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} class }{\pageref{classmlpack_1_1neighbor_1_1NearestNeighborSort}}{}
\item\contentsline{section}{{\bf mlpack\-::neighbor\-::\-Neighbor\-Search$<$ Sort\-Policy, Metric\-Type, Tree\-Type $>$} \\*The \doxyref{Neighbor\-Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} class is a template class for performing distance-\/based neighbor searches }{\pageref{classmlpack_1_1neighbor_1_1NeighborSearch}}{}
\item\contentsline{section}{{\bf mlpack\-::neighbor\-::\-Neighbor\-Search\-Rules$<$ Sort\-Policy, Metric\-Type, Tree\-Type $>$} }{\pageref{classmlpack_1_1neighbor_1_1NeighborSearchRules}}{}
\item\contentsline{section}{{\bf mlpack\-::neighbor\-::\-Neighbor\-Search\-Stat$<$ Sort\-Policy $>$} \\*Extra data for each node in the tree }{\pageref{classmlpack_1_1neighbor_1_1NeighborSearchStat}}{}
\item\contentsline{section}{{\bf mlpack\-::neighbor\-::\-Neighbor\-Search\-Traversal\-Info$<$ Tree\-Type $>$} \\*Traversal information for \doxyref{Neighbor\-Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} }{\pageref{classmlpack_1_1neighbor_1_1NeighborSearchTraversalInfo}}{}
\item\contentsline{section}{{\bf mlpack\-::neighbor\-::\-R\-A\-Search\-Rules$<$ Sort\-Policy, Metric\-Type, Tree\-Type $>$} }{\pageref{classmlpack_1_1neighbor_1_1RASearchRules}}{}
\item\contentsline{section}{{\bf mlpack\-::nn\-::\-Sparse\-Autoencoder$<$ Optimizer\-Type $>$} \\*A sparse autoencoder is a neural network whose aim to learn compressed representations of the data, typically for dimensionality reduction, with a constraint on the activity of the neurons in the network }{\pageref{classmlpack_1_1nn_1_1SparseAutoencoder}}{}
\item\contentsline{section}{{\bf mlpack\-::nn\-::\-Sparse\-Autoencoder\-Function} \\*This is a class for the sparse autoencoder objective function }{\pageref{classmlpack_1_1nn_1_1SparseAutoencoderFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-Aug\-Lagrangian$<$ Lagrangian\-Function $>$} \\*The \doxyref{Aug\-Lagrangian}{p.}{classmlpack_1_1optimization_1_1AugLagrangian} class implements the Augmented Lagrangian method of optimization }{\pageref{classmlpack_1_1optimization_1_1AugLagrangian}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-Aug\-Lagrangian\-Function$<$ Lagrangian\-Function $>$} \\*This is a utility class used by \doxyref{Aug\-Lagrangian}{p.}{classmlpack_1_1optimization_1_1AugLagrangian}, meant to wrap a Lagrangian\-Function into a function usable by a simple optimizer like L-\/\-B\-F\-G\-S }{\pageref{classmlpack_1_1optimization_1_1AugLagrangianFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-Aug\-Lagrangian\-Test\-Function} \\*This function is taken from \char`\"{}\-Practical Mathematical Optimization\char`\"{} (Snyman), section 5.\-3.\-8 (\char`\"{}\-Application of the Augmented Lagrangian Method\char`\"{}) }{\pageref{classmlpack_1_1optimization_1_1AugLagrangianTestFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-Exponential\-Schedule} \\*The exponential cooling schedule cools the temperature T at every step according to the equation }{\pageref{classmlpack_1_1optimization_1_1ExponentialSchedule}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-Gockenbach\-Function} \\*This function is taken from M }{\pageref{classmlpack_1_1optimization_1_1GockenbachFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-L\-\_\-\-B\-F\-G\-S$<$ Function\-Type $>$} \\*The generic L-\/\-B\-F\-G\-S optimizer, which uses a back-\/tracking line search algorithm to minimize a function }{\pageref{classmlpack_1_1optimization_1_1L__BFGS}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-Lovasz\-Theta\-S\-D\-P} \\*This function is the Lovasz-\/\-Theta semidefinite program, as implemented in the following paper\-: }{\pageref{classmlpack_1_1optimization_1_1LovaszThetaSDP}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-L\-R\-S\-D\-P} \\*\doxyref{L\-R\-S\-D\-P}{p.}{classmlpack_1_1optimization_1_1LRSDP} is the implementation of Monteiro and Burer's formulation of low-\/rank semidefinite programs (L\-R-\/\-S\-D\-P) }{\pageref{classmlpack_1_1optimization_1_1LRSDP}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-L\-R\-S\-D\-P\-Function} \\*The objective function that \doxyref{L\-R\-S\-D\-P}{p.}{classmlpack_1_1optimization_1_1LRSDP} is trying to optimize }{\pageref{classmlpack_1_1optimization_1_1LRSDPFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-S\-A$<$ Function\-Type, Cooling\-Schedule\-Type $>$} \\*Simulated Annealing is an stochastic optimization algorithm which is able to deliver near-\/optimal results quickly without knowing the gradient of the function being optimized }{\pageref{classmlpack_1_1optimization_1_1SA}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::\-S\-G\-D$<$ Decomposable\-Function\-Type $>$} \\*Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum of other functions }{\pageref{classmlpack_1_1optimization_1_1SGD}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::test\-::\-Generalized\-Rosenbrock\-Function} \\*The Generalized Rosenbrock function in n dimensions, defined by f(x) = sum\-\_\-i$^\wedge$\{n -\/ 1\} (f(i)(x)) f\-\_\-i(x) = 100 $\ast$ (x\-\_\-i$^\wedge$2 -\/ x\-\_\-\{i + 1\})$^\wedge$2 + (1 -\/ x\-\_\-i)$^\wedge$2 x\-\_\-0 = [-\/1.\-2, 1, -\/1.\-2, 1, ...] }{\pageref{classmlpack_1_1optimization_1_1test_1_1GeneralizedRosenbrockFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::test\-::\-Rosenbrock\-Function} \\*The Rosenbrock function, defined by f(x) = f1(x) + f2(x) f1(x) = 100 (x2 -\/ x1$^\wedge$2)$^\wedge$2 f2(x) = (1 -\/ x1)$^\wedge$2 x\-\_\-0 = [-\/1.\-2, 1] }{\pageref{classmlpack_1_1optimization_1_1test_1_1RosenbrockFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::test\-::\-Rosenbrock\-Wood\-Function} \\*The Generalized Rosenbrock function in 4 dimensions with the Wood Function in four dimensions }{\pageref{classmlpack_1_1optimization_1_1test_1_1RosenbrockWoodFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::test\-::\-S\-G\-D\-Test\-Function} \\*Very, very simple test function which is the composite of three other functions }{\pageref{classmlpack_1_1optimization_1_1test_1_1SGDTestFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::optimization\-::test\-::\-Wood\-Function} \\*The Wood function, defined by f(x) = f1(x) + f2(x) + f3(x) + f4(x) + f5(x) + f6(x) f1(x) = 100 (x2 -\/ x1$^\wedge$2)$^\wedge$2 f2(x) = (1 -\/ x1)$^\wedge$2 f3(x) = 90 (x4 -\/ x3$^\wedge$2)$^\wedge$2 f4(x) = (1 -\/ x3)$^\wedge$2 f5(x) = 10 (x2 + x4 -\/ 2)$^\wedge$2 f6(x) = (1 / 10) (x2 -\/ x4)$^\wedge$2 x\-\_\-0 = [-\/3, -\/1, -\/3, -\/1] }{\pageref{classmlpack_1_1optimization_1_1test_1_1WoodFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::\-Param\-Data} \\*Aids in the extensibility of \doxyref{C\-L\-I}{p.}{classmlpack_1_1CLI} by focusing potential changes into one structure }{\pageref{structmlpack_1_1ParamData}}{}
\item\contentsline{section}{{\bf mlpack\-::pca\-::\-P\-C\-A} \\*This class implements principal components analysis (\doxyref{P\-C\-A}{p.}{classmlpack_1_1pca_1_1PCA}) }{\pageref{classmlpack_1_1pca_1_1PCA}}{}
\item\contentsline{section}{{\bf mlpack\-::perceptron\-::\-Perceptron$<$ Learn\-Policy, Weight\-Initialization\-Policy, Mat\-Type $>$} \\*This class implements a simple perceptron (i.\-e., a single layer neural network) }{\pageref{classmlpack_1_1perceptron_1_1Perceptron}}{}
\item\contentsline{section}{{\bf mlpack\-::perceptron\-::\-Random\-Initialization} \\*This class is used to initialize weights for the weight\-Vectors matrix in a random manner }{\pageref{classmlpack_1_1perceptron_1_1RandomInitialization}}{}
\item\contentsline{section}{{\bf mlpack\-::perceptron\-::\-Simple\-Weight\-Update} }{\pageref{classmlpack_1_1perceptron_1_1SimpleWeightUpdate}}{}
\item\contentsline{section}{{\bf mlpack\-::perceptron\-::\-Zero\-Initialization} \\*This class is used to initialize the matrix weight\-Vectors to zero }{\pageref{classmlpack_1_1perceptron_1_1ZeroInitialization}}{}
\item\contentsline{section}{{\bf mlpack\-::radical\-::\-Radical} \\*An implementation of R\-A\-D\-I\-C\-A\-L, an algorithm for independent component analysis (I\-C\-A) }{\pageref{classmlpack_1_1radical_1_1Radical}}{}
\item\contentsline{section}{{\bf mlpack\-::range\-::\-Range\-Search$<$ Metric\-Type, Tree\-Type $>$} \\*The \doxyref{Range\-Search}{p.}{classmlpack_1_1range_1_1RangeSearch} class is a template class for performing range searches }{\pageref{classmlpack_1_1range_1_1RangeSearch}}{}
\item\contentsline{section}{{\bf mlpack\-::range\-::\-Range\-Search\-Rules$<$ Metric\-Type, Tree\-Type $>$} }{\pageref{classmlpack_1_1range_1_1RangeSearchRules}}{}
\item\contentsline{section}{{\bf mlpack\-::range\-::\-Range\-Search\-Stat} \\*Statistic class for \doxyref{Range\-Search}{p.}{classmlpack_1_1range_1_1RangeSearch}, to be set to the Statistic\-Type of the tree type that range search is being performed with }{\pageref{classmlpack_1_1range_1_1RangeSearchStat}}{}
\item\contentsline{section}{{\bf mlpack\-::regression\-::\-L\-A\-R\-S} \\*An implementation of \doxyref{L\-A\-R\-S}{p.}{classmlpack_1_1regression_1_1LARS}, a stage-\/wise homotopy-\/based algorithm for l1-\/regularized linear regression (L\-A\-S\-S\-O) and l1+l2 regularized linear regression (Elastic Net) }{\pageref{classmlpack_1_1regression_1_1LARS}}{}
\item\contentsline{section}{{\bf mlpack\-::regression\-::\-Linear\-Regression} \\*A simple linear regression algorithm using ordinary least squares }{\pageref{classmlpack_1_1regression_1_1LinearRegression}}{}
\item\contentsline{section}{{\bf mlpack\-::regression\-::\-Logistic\-Regression$<$ Optimizer\-Type $>$} }{\pageref{classmlpack_1_1regression_1_1LogisticRegression}}{}
\item\contentsline{section}{{\bf mlpack\-::regression\-::\-Logistic\-Regression\-Function} \\*The log-\/likelihood function for the logistic regression objective function }{\pageref{classmlpack_1_1regression_1_1LogisticRegressionFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::sparse\-\_\-coding\-::\-Data\-Dependent\-Random\-Initializer} \\*A data-\/dependent random dictionary initializer for \doxyref{Sparse\-Coding}{p.}{classmlpack_1_1sparse__coding_1_1SparseCoding} }{\pageref{classmlpack_1_1sparse__coding_1_1DataDependentRandomInitializer}}{}
\item\contentsline{section}{{\bf mlpack\-::sparse\-\_\-coding\-::\-Nothing\-Initializer} \\*A Dictionary\-Initializer for \doxyref{Sparse\-Coding}{p.}{classmlpack_1_1sparse__coding_1_1SparseCoding} which does not initialize anything; it is useful for when the dictionary is already known and will be set with \doxyref{Sparse\-Coding\-::\-Dictionary()}{p.}{classmlpack_1_1sparse__coding_1_1SparseCoding_a67d822a3a76a44d6402c55f45edb67ad} }{\pageref{classmlpack_1_1sparse__coding_1_1NothingInitializer}}{}
\item\contentsline{section}{{\bf mlpack\-::sparse\-\_\-coding\-::\-Random\-Initializer} \\*A Dictionary\-Initializer for use with the \doxyref{Sparse\-Coding}{p.}{classmlpack_1_1sparse__coding_1_1SparseCoding} class }{\pageref{classmlpack_1_1sparse__coding_1_1RandomInitializer}}{}
\item\contentsline{section}{{\bf mlpack\-::sparse\-\_\-coding\-::\-Sparse\-Coding$<$ Dictionary\-Initializer $>$} \\*An implementation of Sparse Coding with Dictionary Learning that achieves sparsity via an l1-\/norm regularizer on the codes (L\-A\-S\-S\-O) or an (l1+l2)-\/norm regularizer on the codes (the Elastic Net) }{\pageref{classmlpack_1_1sparse__coding_1_1SparseCoding}}{}
\item\contentsline{section}{{\bf mlpack\-::svd\-::\-Q\-U\-I\-C\-\_\-\-S\-V\-D} }{\pageref{classmlpack_1_1svd_1_1QUIC__SVD}}{}
\item\contentsline{section}{{\bf mlpack\-::svd\-::\-Regularized\-S\-V\-D$<$ Optimizer\-Type $>$} }{\pageref{classmlpack_1_1svd_1_1RegularizedSVD}}{}
\item\contentsline{section}{{\bf mlpack\-::svd\-::\-Regularized\-S\-V\-D\-Function} }{\pageref{classmlpack_1_1svd_1_1RegularizedSVDFunction}}{}
\item\contentsline{section}{{\bf mlpack\-::\-Timer} \\*The timer class provides a way for M\-L\-P\-A\-C\-K methods to be timed }{\pageref{classmlpack_1_1Timer}}{}
\item\contentsline{section}{{\bf mlpack\-::\-Timers} }{\pageref{classmlpack_1_1Timers}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Binary\-Space\-Tree$<$ Bound\-Type, Statistic\-Type, Mat\-Type, Split\-Type $>$} \\*A binary space partitioning tree, such as a K\-D-\/tree or a ball tree }{\pageref{classmlpack_1_1tree_1_1BinarySpaceTree}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Binary\-Space\-Tree$<$ Bound\-Type, Statistic\-Type, Mat\-Type, Split\-Type $>$\-::\-Dual\-Tree\-Traverser$<$ Rule\-Type $>$} \\*A dual-\/tree traverser for binary space trees; see dual\-\_\-tree\-\_\-traverser.\-hpp }{\pageref{classmlpack_1_1tree_1_1BinarySpaceTree_1_1DualTreeTraverser}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Binary\-Space\-Tree$<$ Bound\-Type, Statistic\-Type, Mat\-Type, Split\-Type $>$\-::\-Single\-Tree\-Traverser$<$ Rule\-Type $>$} \\*A single-\/tree traverser for binary space trees; see single\-\_\-tree\-\_\-traverser.\-hpp for implementation }{\pageref{classmlpack_1_1tree_1_1BinarySpaceTree_1_1SingleTreeTraverser}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Compare\-Cosine\-Node} }{\pageref{classmlpack_1_1tree_1_1CompareCosineNode}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Cosine\-Tree} }{\pageref{classmlpack_1_1tree_1_1CosineTree}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Cover\-Tree$<$ Metric\-Type, Root\-Point\-Policy, Statistic\-Type $>$} \\*A cover tree is a tree specifically designed to speed up nearest-\/neighbor computation in high-\/dimensional spaces }{\pageref{classmlpack_1_1tree_1_1CoverTree}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Cover\-Tree$<$ Metric\-Type, Root\-Point\-Policy, Statistic\-Type $>$\-::\-Dual\-Tree\-Traverser$<$ Rule\-Type $>$} \\*A dual-\/tree cover tree traverser; see dual\-\_\-tree\-\_\-traverser.\-hpp }{\pageref{classmlpack_1_1tree_1_1CoverTree_1_1DualTreeTraverser}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Cover\-Tree$<$ Metric\-Type, Root\-Point\-Policy, Statistic\-Type $>$\-::\-Dual\-Tree\-Traverser$<$ Rule\-Type $>$\-::\-Dual\-Cover\-Tree\-Map\-Entry} \\*Struct used for traversal }{\pageref{structmlpack_1_1tree_1_1CoverTree_1_1DualTreeTraverser_1_1DualCoverTreeMapEntry}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Cover\-Tree$<$ Metric\-Type, Root\-Point\-Policy, Statistic\-Type $>$\-::\-Single\-Tree\-Traverser$<$ Rule\-Type $>$} \\*A single-\/tree cover tree traverser; see single\-\_\-tree\-\_\-traverser.\-hpp for implementation }{\pageref{classmlpack_1_1tree_1_1CoverTree_1_1SingleTreeTraverser}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Empty\-Statistic} \\*Empty statistic if you are not interested in storing statistics in your tree }{\pageref{classmlpack_1_1tree_1_1EmptyStatistic}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Example\-Tree$<$ Metric\-Type, Statistic\-Type, Mat\-Type $>$} \\*This is not an actual space tree but instead an example tree that exists to show and document all the functions that mlpack trees must implement }{\pageref{classmlpack_1_1tree_1_1ExampleTree}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-First\-Point\-Is\-Root} \\*This class is meant to be used as a choice for the policy class Root\-Point\-Policy of the \doxyref{Cover\-Tree}{p.}{classmlpack_1_1tree_1_1CoverTree} class }{\pageref{classmlpack_1_1tree_1_1FirstPointIsRoot}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Mean\-Split$<$ Bound\-Type, Mat\-Type $>$} \\*A binary space partitioning tree node is split into its left and right child }{\pageref{classmlpack_1_1tree_1_1MeanSplit}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-M\-R\-K\-D\-Statistic} \\*Statistic for multi-\/resolution kd-\/trees }{\pageref{classmlpack_1_1tree_1_1MRKDStatistic}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Tree\-Traits$<$ Tree\-Type $>$} \\*The \doxyref{Tree\-Traits}{p.}{classmlpack_1_1tree_1_1TreeTraits} class provides compile-\/time information on the characteristics of a given tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Tree\-Traits$<$ Binary\-Space\-Tree$<$ Bound\-Type, Statistic\-Type, Mat\-Type $>$ $>$} \\*This is a specialization of the Tree\-Type class to the \doxyref{Binary\-Space\-Tree}{p.}{classmlpack_1_1tree_1_1BinarySpaceTree} tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01BinarySpaceTree_3_01BoundType_00_01StatisticType_00_01MatType_01_4_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::tree\-::\-Tree\-Traits$<$ Cover\-Tree$<$ Metric\-Type, Root\-Point\-Policy, Statistic\-Type $>$ $>$} \\*The specialization of the \doxyref{Tree\-Traits}{p.}{classmlpack_1_1tree_1_1TreeTraits} class for the \doxyref{Cover\-Tree}{p.}{classmlpack_1_1tree_1_1CoverTree} tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01CoverTree_3_01MetricType_00_01RootPointPolicy_00_01StatisticType_01_4_01_4}}{}
\item\contentsline{section}{{\bf mlpack\-::util\-::\-C\-L\-I\-Deleter} \\*Extremely simple class whose only job is to delete the existing \doxyref{C\-L\-I}{p.}{classmlpack_1_1CLI} object at the end of execution }{\pageref{classmlpack_1_1util_1_1CLIDeleter}}{}
\item\contentsline{section}{{\bf mlpack\-::util\-::\-Null\-Out\-Stream} \\*Used for \doxyref{Log\-::\-Debug}{p.}{classmlpack_1_1Log_a771c6107b594176ffae1a036c253ca93} when not compiled with debugging symbols }{\pageref{classmlpack_1_1util_1_1NullOutStream}}{}
\item\contentsline{section}{{\bf mlpack\-::util\-::\-Option$<$ N $>$} \\*A static object whose constructor registers a parameter with the \doxyref{C\-L\-I}{p.}{classmlpack_1_1CLI} class }{\pageref{classmlpack_1_1util_1_1Option}}{}
\item\contentsline{section}{{\bf mlpack\-::util\-::\-Prefixed\-Out\-Stream} \\*Allows us to output to an ostream with a prefix at the beginning of each line, in the same way we would output to cout or cerr }{\pageref{classmlpack_1_1util_1_1PrefixedOutStream}}{}
\item\contentsline{section}{{\bf mlpack\-::util\-::\-Program\-Doc} \\*A static object whose constructor registers program documentation with the \doxyref{C\-L\-I}{p.}{classmlpack_1_1CLI} class }{\pageref{classmlpack_1_1util_1_1ProgramDoc}}{}
\item\contentsline{section}{{\bf mlpack\-::util\-::\-Save\-Restore\-Utility} }{\pageref{classmlpack_1_1util_1_1SaveRestoreUtility}}{}
\item\contentsline{section}{{\bf R\-A\-Search$<$ Sort\-Policy, Metric\-Type, Tree\-Type $>$} \\*The \doxyref{R\-A\-Search}{p.}{classRASearch} class\-: This class provides a generic manner to perform rank-\/approximate search via random-\/sampling }{\pageref{classRASearch}}{}
\item\contentsline{section}{{\bf Traversal\-Info$<$ Tree\-Type $>$} \\*The \doxyref{Traversal\-Info}{p.}{classTraversalInfo} class holds traversal information which is used in dual-\/tree (and single-\/tree) traversals }{\pageref{classTraversalInfo}}{}
\end{DoxyCompactList}
